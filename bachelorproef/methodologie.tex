%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Methodologie}{Methodology}}
\label{ch:methodologie}

%% TODO: Hoe ben je te werk gegaan? Verdeel je onderzoek in grote fasen, en
%% licht in elke fase toe welke stappen je gevolgd hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent. Je moet kunnen aantonen dat je de best
%% mogelijke manier toegepast hebt om een antwoord te vinden op de
%% onderzoeksvraag.

\section{Berekeningen in \textit{back-end}}
%\subsection{Algemeen}
Na het literatuuronderzoek dat uitgeschreven is in de stand van zaken over wat \textit{elderspeak} en \textit{nursery tone} precies zijn, wat AI is en welke types er zijn, wat en hoe \textit{speech recognition} werkt en hoe men achtergrond lawaai filtert, zijn we toe aan de \textit{proof-of-concept}. Deze voorbeeld applicatie, geschreven in Python en gebruikmakend van het \textit{micro-framework} Flask, is een webapplicatie die verschillende pagina’s heeft. Naast de inleidende pagina, zie schermafbeelding X, bevat deze ook een detector, zie schermafbeelding X, en een oplijsting van wat \textit{elderspeak} is, zie schermafbeelding X. Op die manier kunnen mensen, maar specifiek studenten in de zorg, actief en passief leren wat \textit{elderspeak} precies is. Enerzijds kunnen ze de eigenschappen te weten komen door zelf actief stukjes spraak op te nemen, te laten analyseren zodat men kan zien welke eigenschappen er aanwezig waren. Anderzijds kunnen ze passief bekijken wat de eigenschappen zijn van \textit{elderspeak} en hoe men dat kan voorkomen.

De applicatie werd zo gemaakt waarbij men eerst twee jonge vrouwen ziet, een foto van HoGent voor de copyright, waarbij er gevraagd wordt om te spreken zoals men zou doen tegen hen. Eens dat geluid geanalyseerd is, zal er een foto van een oudere vrouw getoond wordt in het rusthuis waarbij er gevraagd wordt om tegen haar te spreken. Hierdoor kan de applicatie eerst de stemfrequentie en -volume berekenen zodat deze gebruikt kunnen worden om te vergelijken met het geluid van de 2\textsuperscript{e} casus.

\subsection{\textit{Speech recognition}}
De basis van een groot deel van de applicatie is het herkennen van de spraak die wordt opgenomen op de website. Uiteraard moet dit niet van nul gemaakt worden, maar kan er gebruik gemaakt worden van bestaande Python-bibliotheken. \textcite{Standaert2021} bestudeerde in zijn bachelorproef afgelopen jaar dat de beste optie is om de Google Speech Recognition-API te gebruiken omdat deze het beste presteert. Hij vatte dit samen in zijn conclusie: ``Uit verschillende onderzoeken of studies waar men verschillende ASR-systemen met elkaar vergeleken kwam de Google Speech-API er altijd als beste uit. En dit in alle aspecten.''. Daarnaast gebruikt de Google Speech-API ook \textit{natural language processing} of NLP om beter te kunnen weten wat er precies gezegd is geweest \autocite{GoogleCloud2022}. Hoe dit precies in elkaar zit is te lezen in het hoofdstuk `stand van zaken'.

Bij de spraakherkenning kwam er wel een groot probleem de kop op steken. De Google Speech Recognition-API kon alleen overweg met \textit{wav}- en \textit{flac}-bestanden én kan maar een bepaalde periode, ongeveer 2 tot 3 minuten, gratis gaan herkennen, maar de audio werd gecapteerd in \textit{mp3}-bestanden. Hiervoor was het noodzakelijk om eerst een conversie te doen van \textit{mp3} naar \textit{flac} en om het audiobestand op te delen in deelbestandjes of \textit{chuncks}, zodat er geen betalende versie voor nodig was. De gebruikte technologie hierbij was ``ffmpeg''. Hoe de conversie precies gebeurt is te vinden in bijlage X.

Het gebruiken van die API is relatief gemakkelijk te gebruiken. Een extra optie is dan ook ter beschikking om achtergrondlawaai een beetje weg te filteren. Door de optie `ajust\_for\_ambient\_noice(source)' aan te zetten, zal de bibliotheek zich wat aanpassen aan de geluidbron zodat het beter de klanken kan horen alvorens die worden doorgestuurd naar de spraakherkenning-API. Deze methode gebruikt de techniek die beschreven is in het literatuuronderzoek over achtergrondlawaai.

Eens dit ingesteld is worden de audiobestanden mee gegeven en krijgt het systeem de tekst terug waarvan het AI-model van Google denkt dat het die woorden en zinnen zijn. Uiteraard werkt dit niet feilloos, maar het is wel redelijk goed voor een gratis versie. Met deze grotere methode is de basis gelegd voor andere methoden die steunen op de tekst die herkend werd.

\subsection{Verkleinwoorden}
Voor de methode om verkleinwoorden te herkennen is er gesteund op de methode van \textcite{Standaert2021}. Daarbij wordt er gekeken of woorden langer zijn dan 3 letters en ze niet voorkomen in de lijst van geen verkleinwoorden. Enkele voorbeelden die wel eindigen op ``-je'', ``-ke'', ``-kes'' of ``-jes'', maar geen verkleinwoorden zijn, zijn: poffertje, meisje, koopje, etentje, dutje, toetje, mannelijke, vrouwelijke etc. De code voor deze methode kan gevonden worden in bijlage X
\subsection{Herhalingen}
Om herhalingen te herkennen werd er evenals gebruikt gemaakt van de methode die \textcite{Standaert2021} geschreven had. Daarbij wordt er bijgehouden wat de vorige 25 woorden waren en bij herhalingen worden de woorden die herhaald worden bewaard in een lijst. Die zullen we later gebruiken om een mooie weergave te maken.
De code om herhalingen te detecteren is te vinden in bijlage X

\subsection{Collectieve voornaamwoorden}
Een voorbeeld van een collectief voornaamwoord is ``we''. Om het voorbeeld te verduidelijken zijn de volgende zinnen gegeven: ``Gaan we onze patatjes opeten?'', ``Kunnen we alleen naar de wc?'', ``Awel, wat zijn we aan het doen?''.

Er wordt bijgehouden hoeveel keer er collectieve voornaamwoorden gebruikt worden in de tekst. Als een woord meer dan een keer voorkomt, dan beschouwt de applicatie dit dat het voorkomt. Dit is zo ingesteld omdat het niet mag worden weergegeven wanneer er iemand een keer het woord ``we'' gebruikt.
Wanneer er geen of minder dan 2 collectieve voornaamwoorden gebruikt worden, zal de applicatie zeggen dat er geen of niet genoeg gebruikt werden.

\subsection{Tussenwerpsels}
Het veelvuldig gebruik van tussenwerpsels is een eigenschap van \textit{elderspeak} en ook dit wordt herkend. Enkele voorbeelden van tussenwerpsels die herkend worden zijn: o, oeps, helaas, hallo, hey, voila etc. De Google Speech-API geeft soms verschillende varianten op het woord ``hey''. Zo worden de volgende vormen soms gegeven: hé, hè, he, hey. Om te voorkomen dat foute resultaten geeft, worden al deze varianten herleid naar ``hey''.

De werkwijze om dit te detecteren is ongeveer dezelfde als bij de methode van de collectieve voornaamwoorden die te vinden is in bijlage X.

\subsection{Stemfrequentie}
Stemfrequentie is een bijzonder belangrijke eigenschap van \textit{elderspeak}. Deze eigenschap is ook volledig onafhankelijk van de gesproken tekst die herkend werd. Wanneer een persoon merkbaar hoger zal praten, dan zal de andere persoon zich direct voelen dat hij/zij behandelt wordt als een kind. Het is dan ook belangrijk dat deze functie goed werkt zodat de gebruikers direct weten als ze (on)bewust hoger praten.

Deze methode werd al opgesteld door \textcite{Standaert2021} in zijn eindwerk van het vorige jaar. Wat hij berekende was de gemiddelde frequentie, uitgedrukt in Hz, van het gegeven audiobestand. Wat er aan deze applicatie toegevoegd is, is het berekenen of de frequentie hoger ligt bij de casus met de oudere vrouw dan de casus bij de twee jonger vrouwen.
Hoe dit gerealiseerd werd in de code is te vinden in bijlage X.

\subsection{Stemvolume}
De laatste eigenschap die geanalyseerd wordt in de applicatie is om te controleren of er luider gesproken wordt in het 2\textsuperscript{e} fragment dan in het 1\textsuperscript{ste}. Toch moet er hier een duidelijke kanttekening bij gemaakt worden. Wanneer een persoon bij de 2\textsuperscript{e} opname significant verder van de microfoon staat, zal de applicatie dit detecteren dat het niet luider zal zijn. Daarnaast moet er ook geweten zijn dat de meeste oudere mensen slechthorend zijn, waardoor men wel luider moet praten. Ondanks deze twee zaken vond ik het toch belangrijk om deze eigenschap te implementeren zodat men er wel eens bij stil staat dat niet iedereen slechthorend is of een hoorapparaat draagt.

Om een getal te verkrijgen dat het volume voorstelt, is er gebruik gemaakt van de pyln-bibliotheek die een BS.170 geluidmeter aanmaakt. Deze analyseert dan de audio en geeft een getal weer. Hoe dit precies geïmplementeerd werd is te vinden in bijlage X.


\section{\textit{Front-end}}
\subsection{Geluid opnemen}
Het geluid opnemen gebeurt volledig aan de kant van de \textit{client} of de gebruiker. Wanneer er op de knop wordt geduwd om de audio-opname te starten, zullen er \textit{audiochunks } worden toegevoegd aan een lijst. Die worden na de opname allemaal samengevoegd tot een \textit{blob}, of een \textit{binary-large object}, die dan een mp3-file aanmaakt.
Per casus wordt er ook een andere afbeelding en tekst getond boven de opneemknop.

\subsection{API-afhandeling}
Vanuit de front-end worden er \textit{API-requests} gestuurd met het audiobestand als bijlage. De \textit{back-end} als server krijgt dan het geluid binnen. Dat wordt dan door verschillende methodes geanalyseerd. Wanneer alles onderzocht is wordt alle data verzameld en via een JSON terug gestuurd naar de \textit{client}. Daar worden de resultaten ingevuld in de voorziene html-stukken.


\section{Testen}
Om een objectief beeld te kunnen verkrijgen hoe goed de applicatie werkt, zijn er automatische testen opgezet. De data die verzameld is via een online formulier, te vinden op:  `\url{https://www.jotform.com/form/213524968382060}', in het begin van het twee semester, werd allemaal opgeslagen. Eens de data opgeslagen was werd alle data beluisterd en handmatig gelabeld.

Nadien werd er een Python script gemaakt die het testen van al die 54 bestanden automatiseerde. De audiobestanden die geklasseerd werden waarbij er geen \textit{elderspeak} aanwezig bij was, werden gebruikt zoals in de echte webapplicatie om het geluid te normaliseren. Nadien werden de geluidsopnames waarbij er wel \textit{elderspeak} bij aanwezig was gebruikt om het te testen. De resultaten werden dan vergelijken met de gelabelde data.

Met deze resultaten konden er op de eigenschappen verkleinwoorden, hogere frequentie en hoger volume een \texit{confusion matrix} gemaakt worden. Hierbij wordt er bepaald hoeveel correct negatieve, correct positieve, valse positieve en vals negatieve er aanwezig waren in het testset.

\section{hosting?}
